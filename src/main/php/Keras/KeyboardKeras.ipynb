{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/gtk-2.0/gtk/__init__.py:127: RuntimeWarning: PyOS_InputHook is not available for interactive use of PyGTK\n",
      "  set_interactive(1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import Xlib.display as display\n",
    "import gtk, pygtk\n",
    "import csv\n",
    "import numpy as np\n",
    "import ConfigParser\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = 3.5e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read write project path\n",
    "#TODO: configure this file according to your path: BehavioralAuthentication/src/main/php/configuration.cfg\n",
    "config = ConfigParser.RawConfigParser()\n",
    "config.read('configuration.cfg')\n",
    "\n",
    "#project_dir_path = config.get('Section1', 'project_dir')\n",
    "#resource_path = project_dir_path + \"src/test/resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window = gtk.Window()\n",
    "screen = window.get_screen()\n",
    "\n",
    "x_size = screen.get_width()\n",
    "y_size = screen.get_height()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new\n",
    "\n",
    "#ted dunning model\n",
    "\n",
    "def extractFeatures(filename, n_last_timestamps_as_features = 200, normalize = True, bias = True):\n",
    "    queue = {}\n",
    "    \n",
    "    diffTime = {}\n",
    "    oldTime = -1.0\n",
    "    \n",
    "    maxX = 0.0\n",
    "    \n",
    "    x_width = float(screen.get_width()) \n",
    "    y_width = float(screen.get_height()) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    x_attribute = -1\n",
    "    y_attribute = -1\n",
    "    i = 0\n",
    "    r = 0\n",
    "    \n",
    "    record_number = len(list(csv.reader(open(filename))))\n",
    "    print record_number\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "         spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')   \n",
    "            \n",
    "         for row in spamreader:\n",
    "            \n",
    "            feature_num = len(row)\n",
    "            \n",
    "            data_size = record_number - n_last_timestamps_as_features - 1\n",
    "            \n",
    "            if (i==0):\n",
    "                \n",
    "                data = np.zeros((data_size, n_last_timestamps_as_features, feature_num))\n",
    "                target = np.zeros((data_size, feature_num))\n",
    "                \n",
    "                #find mouse position coordinate columns in the file\n",
    "                for a in range(feature_num):\n",
    "                    if (row[a] == \"mouse_X\"):\n",
    "                        x_attribute = a\n",
    "                    if (row[a] == \"mouse_Y\"):\n",
    "                        y_attribute = a\n",
    "                        \n",
    "            else:\n",
    "                #normalize mouse coordinates\n",
    "                if (normalize):\n",
    "                     row[x_attribute] = float(row[x_attribute]) / x_width   \n",
    "                     row[y_attribute] = float(row[y_attribute]) / y_width\n",
    "                \n",
    "                \n",
    "                if (i <= n_last_timestamps_as_features):\n",
    "                    data [0,(i-1),:] = row\n",
    "                    \n",
    "                elif(i == n_last_timestamps_as_features + 1):\n",
    "                    target[0,:] = row\n",
    "                    \n",
    "                    r = r + 1\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    data[r, 0 : (n_last_timestamps_as_features - 1), :] = data[(r - 1), 1 : n_last_timestamps_as_features, :]\n",
    "                    data[r, (n_last_timestamps_as_features - 1), :] = target[(r - 1), :]\n",
    "                        \n",
    "                    target[r,:] = row                   \n",
    "                \n",
    "                    r = r + 1\n",
    "            i = i + 1\n",
    "            \n",
    "    #if (bias):\n",
    "    #    data = np.hstack((data, np.ones((data.shape[0], 1), dtype=data.dtype)))\n",
    "    \n",
    "    \n",
    "    print(\"original shape: \" + str(data.shape))\n",
    "    \n",
    "\n",
    "    return ((data, target))\n",
    "            \n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "original shape: (6, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  1.,   2.],\n",
       "         [  3.,   4.]],\n",
       " \n",
       "        [[  3.,   4.],\n",
       "         [  5.,   6.]],\n",
       " \n",
       "        [[  5.,   6.],\n",
       "         [  7.,   8.]],\n",
       " \n",
       "        [[  7.,   8.],\n",
       "         [  9.,  10.]],\n",
       " \n",
       "        [[  9.,  10.],\n",
       "         [ 11.,  12.]],\n",
       " \n",
       "        [[ 11.,  12.],\n",
       "         [ 13.,  14.]]]), array([[  5.,   6.],\n",
       "        [  7.,   8.],\n",
       "        [  9.,  10.],\n",
       "        [ 11.,  12.],\n",
       "        [ 13.,  14.],\n",
       "        [ 15.,  16.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractFeatures(\"/home/felix/mousetracking/BehavioralAuthentication/src/main/java/features4.txt\", \\\n",
    "                n_last_timestamps_as_features = 2, normalize = False, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateModelData((data1,target1), n_last_timestamps_as_features = 200, \n",
    "                      v = False, build_all_model = True, p_train_general = 1.0, \n",
    "                      stddev_factor = 1.0, returnSmoothed = False):\n",
    "    \n",
    "    N_total = data1.shape[0]\n",
    "    N_total_test = N_total * 0.4\n",
    "    \n",
    "    #(data,target) = getMovementsOnly(data1, target1)\n",
    "    (data,target) = (data1, target1)\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    print data.shape\n",
    "    print target.shape\n",
    "    \n",
    "    p_train = 0.6\n",
    "    X_train = data[0:(N * p_train_general * p_train),:]\n",
    "    y_train = target[0:(N * p_train_general * p_train),:]\n",
    "    X_test = data[((N * p_train) + n_last_timestamps_as_features):N,:]\n",
    "    y_test = target[((N * p_train) + n_last_timestamps_as_features):N,:]\n",
    "    \n",
    "    N_total_test = X_test.shape[0]\n",
    "    \n",
    "\n",
    "    regr = linear_model.LinearRegression(normalize=False)\n",
    "    \n",
    "    dist_train = getDistance(y_train)\n",
    "    \n",
    "    #impossible threshold\n",
    "    impossible_threshold = np.array([stddev_factor, stddev_factor])\n",
    "\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train);\n",
    "    \n",
    "    #training error\n",
    "    dist_train = getDistance(y_train)\n",
    "    \n",
    "    prediction_train = regr.predict(X_train)\n",
    "    \n",
    "    #target y is constrained to be within in [0,1]\n",
    "    prediction_train[prediction_train < 0.0] = 0\n",
    "    prediction_train[prediction_train > 1.0] = 1\n",
    "    \n",
    "    diff_train = np.abs(prediction_train - y_train)\n",
    "    diff_x_train = diff_train[:,0]\n",
    "    diff_y_train = diff_train[:,1]\n",
    "    \n",
    "    train_error_mse = (np.sum((np.power(diff_x_train, 2) + np.power(diff_y_train, 2))) / diff_x_train.shape[0])\n",
    "    train_error_dist = (np.sum((np.power(diff_x_train, 2) + np.power(diff_y_train, 2))) / dist_train)\n",
    "    \n",
    "    (smoothed_time_error_train, smoothed_distance_error_train, N_smoothed_train) = \\\n",
    "    get_smoothed_result(X_train,y_train, diff_train, impossible_threshold, v = False) \n",
    "   \n",
    "\n",
    "    #test error\n",
    "    dist_test = getDistance(y_test)\n",
    "    \n",
    "    prediction_test = regr.predict(X_test)\n",
    "    \n",
    "    #target y is constrained to be within in [0,1]\n",
    "    prediction_test[prediction_test < 0.0] = 0\n",
    "    prediction_test[prediction_test > 1.0] = 1\n",
    "    \n",
    "    diff_test = np.abs(prediction_test - y_test)        \n",
    "    diff_x_test = diff_test[:,0]\n",
    "    diff_y_test = diff_test[:,1]\n",
    "    \n",
    "    test_error_mse = (np.sum(np.power(diff_x_test, 2) + np.power(diff_y_test, 2)) / diff_x_test.shape[0])\n",
    "    test_error_dist = (np.sum(np.power(diff_x_test, 2) + np.power(diff_y_test, 2)) / dist_test)\n",
    "    \n",
    "    (smoothed_time_error_test, smoothed_distance_error_test, N_smoothed) = \\\n",
    "    get_smoothed_result(X_test,y_test, diff_test, impossible_threshold, v = v) \n",
    "    \n",
    "    if (v):\n",
    "        \n",
    "        print(\"threshold: \" + str(impossible_threshold))\n",
    "           \n",
    "        #Prediction of x-coordinate of the cursor over time\n",
    "        plt.plot(y_test[:,0])\n",
    "        plt.plot(prediction_test[:,0])\n",
    "        plt.ylabel('relative postion - x coordinate of the cursor')\n",
    "        plt.title(\"regression - x\")\n",
    "        plt.show()\n",
    "        \n",
    "        #Prediction of y-coordinate of the cursor over time\n",
    "        plt.plot(y_test[:,1])\n",
    "        plt.plot(prediction_test[:,1])\n",
    "        plt.ylabel('relative postion - y coordinate of the cursor')\n",
    "        plt.title(\"regression - y\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"test - MSE: %.10f\" % test_error_mse)\n",
    "        print(\"test - MSE - dist: %.10f\" % test_error_dist)\n",
    "        \n",
    "        print (\"test - smoothed MSE: \" + str(smoothed_time_error_test))\n",
    "        print (\"test - smoothed MSE per dist: \" + str(smoothed_distance_error_test))\n",
    "        \n",
    "        #Test error over time\n",
    "        threshold_line = np.ones(diff_x_test.shape) * impossible_threshold[0]        \n",
    "        plt.plot(diff_x_test)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Test error')\n",
    "        plt.title(\"ErrorPerTestTime - x\")\n",
    "        plt.show()\n",
    "        threshold_line = np.ones(diff_y_test.shape) * impossible_threshold[1]        \n",
    "        plt.plot(diff_y_test)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Test error')\n",
    "        plt.title(\"ErrorPerTestTime - y\")\n",
    "        plt.show()\n",
    "        \n",
    "        #Training error over time\n",
    "        threshold_line = np.ones(diff_x_train.shape) * impossible_threshold[0]\n",
    "        plt.plot(diff_x_train)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Training error')\n",
    "        plt.title(\"ErrorPerTrainTime - x\")\n",
    "        plt.show()\n",
    "        \n",
    "        threshold_line = np.ones(diff_y_train.shape) * impossible_threshold[1]\n",
    "        plt.plot(diff_y_train)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Training error')\n",
    "        plt.title(\"ErrorPerTrainTime - y\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"train - MSE: %.10f\" % train_error_mse)\n",
    "        print(\"train - MSE - dist: %.10f\" % train_error_dist)    \n",
    "        \n",
    "        print (\"train - smoothed MSE: \" + str(smoothed_time_error_train))\n",
    "        print (\"train - smoothed MSE per dist: \" + str(smoothed_distance_error_train))\n",
    "    \n",
    "        \n",
    "    if (returnSmoothed):\n",
    "        return((smoothed_time_error_test, smoothed_distance_error_test, float(N_smoothed) / float(N_total_test)))\n",
    "    \n",
    "    if (build_all_model):   \n",
    "        final_regr = linear_model.LinearRegression(normalize=True)\n",
    "    \n",
    "        final_regr.fit(data, target);\n",
    "    \n",
    "        return ((final_regr, impossible_threshold))\n",
    "        #return (regr, two_sigma)\n",
    "    else:\n",
    "        return test_error_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateModel(dataFile, n_last_timestamps_as_features = 200, v = False, stddev_factor = 0.0):\n",
    "    (data,target) = extractFeatures(dataFile, n_last_timestamps_as_features)\n",
    "    \n",
    "    return generateModelData((data,target), n_last_timestamps_as_features = n_last_timestamps_as_features, \n",
    "                             v = v, stddev_factor = stddev_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMovementsOnly(data, target):\n",
    "    moveOnlyData = []\n",
    "    moveOnlyTarget = []\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    for l in range(data.shape[0]):\n",
    "        if (first):\n",
    "            \n",
    "            #print data[l]\n",
    "            \n",
    "            moveOnlyData = [data[l]]\n",
    "            moveOnlyTarget = [target[l]]\n",
    "            first = False\n",
    "        else:\n",
    "            \n",
    "            #print (\"new: \" + str(data[l]))\n",
    "            #print (\"old: \" + str(data[l]))\n",
    "            \n",
    "            if (np.any(data[l] != data[l-1]) or np.any(target[l] != target[l-1])):\n",
    "                moveOnlyData = np.concatenate((moveOnlyData, [data[l]]), axis=0)\n",
    "                moveOnlyTarget = np.concatenate((moveOnlyTarget, [target[l]]), axis=0)\n",
    "\n",
    "    return (np.matrix(moveOnlyData), np.matrix(moveOnlyTarget))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean (p1, p2):\n",
    "    return np.sqrt(np.sum(np.power(p1 - p2, 2)))\n",
    "\n",
    "def getDistance(target):\n",
    "    distance = 0.0\n",
    "    for t in np.arange(1, target.shape[0], 1):\n",
    "        distance += euclidean(target[t,:], target[(t-1),:])\n",
    "    \n",
    "    return distance    \n",
    "\n",
    "def getSmoothedDistance(target, isOverThreshold):\n",
    "    \n",
    "    distance = 0.0\n",
    "    for t in np.arange(1, target.shape[0], 1):\n",
    "        last_point_x = target[t-1,0]\n",
    "        last_point_y = target[t-1,1]\n",
    "        if (isOverThreshold[t] == False):\n",
    "            distance += euclidean(target[t,:], np.array([last_point_x,last_point_y])) \n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_smoothed_result(newData, newTarget, diff, threshold, v = False):\n",
    "    \n",
    "    diff_smoothed = np.copy(diff)\n",
    "    \n",
    "    N_smoothed = newTarget.shape[0]\n",
    "    \n",
    "    is_over_threshold = np.logical_or((diff_smoothed[:,0] > threshold[0]),(diff_smoothed[:,1] > threshold[1]))\n",
    "    \n",
    "    newdist = getSmoothedDistance(newTarget, is_over_threshold)\n",
    "    \n",
    "    for l in range(diff_smoothed.shape[0]):\n",
    "        if (is_over_threshold[l]):\n",
    "            diff_smoothed[l,0] = 0\n",
    "            diff_smoothed[l,1] = 0\n",
    "            N_smoothed -= 1\n",
    "            \n",
    "    if (v):\n",
    "        plt.plot(diff_smoothed[:,0])\n",
    "        plt.ylabel('prediction x error -smoothed')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(diff_smoothed[:,1])\n",
    "        plt.ylabel('prediction y error -smoothed')\n",
    "        plt.show()\n",
    "    \n",
    "    time_error = 0.0\n",
    "    if (N_smoothed > 0.0):\n",
    "        time_error = np.sum(np.power(diff_smoothed,2)) / N_smoothed\n",
    "    \n",
    "    distance_error = 0.0\n",
    "    if (newdist > 0.0):\n",
    "        distance_error = np.sum(np.power(diff_smoothed,2)) / newdist\n",
    "    \n",
    "    return ((time_error,distance_error, N_smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showStats(file, m, threshold, n_last_timestamps_as_features = 200, normalize = True, v = True):\n",
    "    (fileX1, fileY1) = extractFeatures(file, \n",
    "                                     n_last_timestamps_as_features = n_last_timestamps_as_features, \n",
    "                                     normalize = normalize) \n",
    "    \n",
    "    return showStatsData((fileX1, fileY1), m, threshold, n_last_timestamps_as_features = n_last_timestamps_as_features, v = v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000086145\n"
     ]
    }
   ],
   "source": [
    "t = 1000000.0\n",
    "    \n",
    "for i in range(1000001):\n",
    "    t += 0.000001\n",
    "\n",
    "t -= 1000000.0\n",
    "\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showStatsData((fileX, fileY), m, threshold, n_last_timestamps_as_features = 200, v = True):\n",
    "    \n",
    "    (newData, newTarget) = getMovementsOnly(fileX, fileY)   \n",
    "    \n",
    "    if newData.shape[0] <= 1: return(0.0, 0.0, 0, 1)\n",
    "    \n",
    "    #distance\n",
    "    dist = getDistance(newTarget)\n",
    "    \n",
    "    prediction_sample = m.predict(newData)\n",
    "    \n",
    "    #target y is constrained to be within in [0,1]\n",
    "    prediction_sample[prediction_sample < 0.0] = 0\n",
    "    prediction_sample[prediction_sample > 1.0] = 1\n",
    "\n",
    "    ErrorPerTestTime = np.sum(np.absolute(prediction_sample - newTarget),1);\n",
    "    \n",
    "    threshold_line = np.ones(ErrorPerTestTime.shape) * (threshold[0] + threshold[1])\n",
    "    \n",
    "    if (v):\n",
    "        #error of sample\n",
    "        plt.plot(ErrorPerTestTime)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('prediction error')\n",
    "        plt.show()\n",
    "\n",
    "        #Prediction of x-coordinate of the cursor over time\n",
    "        plt.plot(prediction_sample[:,0])\n",
    "        plt.plot(newData[:,0])\n",
    "        plt.ylabel('relative x-coordinate')\n",
    "        plt.title(\"Regression for x-coordinate\")    \n",
    "        plt.show()\n",
    "\n",
    "        #Prediction of y-coordinate of the cursor over time\n",
    "        plt.plot(prediction_sample[:,1])\n",
    "        plt.plot(newData[:,1])\n",
    "        plt.ylabel('relative y-coordinate')\n",
    "        plt.title(\"Regression for y-coordinate\")\n",
    "        plt.show()\n",
    "\n",
    "    diff = np.absolute(prediction_sample - newTarget)\n",
    "    \n",
    "    N = newTarget.shape[0]\n",
    "    \n",
    "    if (v):\n",
    "        print (\"MSE per 0.03s: \" + str(np.sum(np.power(diff,2)) / N))\n",
    "        print (\"MSE per pixel move: \" + str(np.sum(np.power(diff,2)) / dist))\n",
    "        print (\"number of records: \" + str(N))\n",
    "\n",
    "        print (\"\\nmax - |x+y|: \" + str(np.max(diff)))\n",
    "        print (\"max - squared: \" + str(np.max(np.power(diff,2))))\n",
    "    \n",
    "    (time_error, distance_error, N_smoothed) = get_smoothed_result(newData, newTarget, diff, threshold)    \n",
    "    \n",
    "    if (v):\n",
    "        print (\"smoothed MSE: \" + str(time_error))\n",
    "        print (\"smoothed MSE per dist: \" + str(distance_error))\n",
    "    \n",
    "    if (time_error > theta): \n",
    "        if (v):\n",
    "            print (\"This is not Felix - You are unauthorized!\")\n",
    "        return (time_error, distance_error, N_smoothed, 0)\n",
    "    else:\n",
    "        if (v):\n",
    "            print (\"This is Felix - Welcome to this awesome computer!\")  \n",
    "        return (time_error, distance_error, N_smoothed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186051\n",
      "original shape: (185950, 100, 2)\n",
      "(185950, 100, 2)\n",
      "(111570, 100, 2)\n",
      "(111570, 2)\n"
     ]
    }
   ],
   "source": [
    "p_train = 0.6\n",
    "p_valid = 0.2\n",
    "#p_test = 0.2\n",
    "\n",
    "rr_steps = 100\n",
    "\n",
    "(data,target) = extractFeatures(\"/home/felix/mousetracking/BehavioralAuthentication/src/main/java/features3.txt\", \\\n",
    "                                rr_steps)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "N = data.shape[0]\n",
    "\n",
    "train_dataset = data[0:(N * p_train),:,:]\n",
    "train_labels = target[0:(N * p_train),:]\n",
    "valid_dataset = data[((N * p_train) + rr_steps):((N * (p_train + p_valid))),:,:]\n",
    "valid_labels = target[((N * p_train) + rr_steps):((N * (p_train + p_valid))),:]\n",
    "test_dataset = data[((N * (p_train + p_valid)) + rr_steps):N,:,:]\n",
    "test_labels = target[((N * (p_train + p_valid)) + rr_steps):N,:]\n",
    "\n",
    "#(train_dataset,train_labels) = getMovementsOnly(train_dataset, train_labels)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "num_features = train_dataset.shape[1]\n",
    "num_labels = train_labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  \n",
    "  return np.sum((np.square(np.matrix(predictions) - np.matrix(labels))).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105991 samples, validate on 5579 samples\n",
      "Epoch 1/10\n",
      "105991/105991 [==============================] - 95s - loss: 0.0536 - val_loss: 0.0258\n",
      "Epoch 2/10\n",
      "105991/105991 [==============================] - 107s - loss: 0.0512 - val_loss: 0.0255\n",
      "Epoch 3/10\n",
      "105991/105991 [==============================] - 102s - loss: 0.0512 - val_loss: 0.0254\n",
      "Epoch 4/10\n",
      "105991/105991 [==============================] - 106s - loss: 0.0511 - val_loss: 0.0253\n",
      "Epoch 5/10\n",
      "105991/105991 [==============================] - 132s - loss: 0.0511 - val_loss: 0.0253\n",
      "Epoch 6/10\n",
      "105991/105991 [==============================] - 116s - loss: 0.0511 - val_loss: 0.0253\n",
      "Epoch 7/10\n",
      "105991/105991 [==============================] - 105s - loss: 0.0511 - val_loss: 0.0252\n",
      "Epoch 8/10\n",
      "105991/105991 [==============================] - 106s - loss: 0.0511 - val_loss: 0.0252\n",
      "Epoch 9/10\n",
      "105991/105991 [==============================] - 105s - loss: 0.0510 - val_loss: 0.0252\n",
      "Epoch 10/10\n",
      "105991/105991 [==============================] - 103s - loss: 0.0510 - val_loss: 0.0252\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from random import random\n",
    "\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Dropout, Activation  \n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "in_neurons = 2  \n",
    "out_neurons = 2  \n",
    "hidden_neurons = 20\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=in_neurons, return_sequences=False)) \n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(output_dim=out_neurons, input_dim=hidden_neurons))\n",
    "model.add(Activation(\"softmax\"))  \n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\") \n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = train_test_split(data)  # retrieve data\n",
    "model.fit(train_dataset, train_labels, batch_size=450, nb_epoch=10, validation_split=0.05)  \n",
    "\n",
    "predicted = model.predict(test_dataset)  \n",
    "rmse = np.sqrt(((predicted - test_labels) ** 2).mean(axis=0))\n",
    "\n",
    "print rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1528915   0.15456401]\n"
     ]
    }
   ],
   "source": [
    "print rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105991 samples, validate on 5579 samples\n",
      "Epoch 1/10\n",
      "105991/105991 [==============================] - 73s - loss: 0.0597 - val_loss: 0.0264\n",
      "Epoch 2/10\n",
      "105991/105991 [==============================] - 98s - loss: 0.0537 - val_loss: 0.0258\n",
      "Epoch 3/10\n",
      "105991/105991 [==============================] - 100s - loss: 0.0531 - val_loss: 0.0257\n",
      "Epoch 4/10\n",
      "105991/105991 [==============================] - 97s - loss: 0.0528 - val_loss: 0.0255\n",
      "Epoch 5/10\n",
      "105991/105991 [==============================] - 104s - loss: 0.0526 - val_loss: 0.0254\n",
      "Epoch 6/10\n",
      "105991/105991 [==============================] - 98s - loss: 0.0525 - val_loss: 0.0254\n",
      "Epoch 7/10\n",
      "105991/105991 [==============================] - 93s - loss: 0.0524 - val_loss: 0.0253\n",
      "Epoch 8/10\n",
      "105991/105991 [==============================] - 102s - loss: 0.0524 - val_loss: 0.0253\n",
      "Epoch 9/10\n",
      "105991/105991 [==============================] - 95s - loss: 0.0523 - val_loss: 0.0253\n",
      "Epoch 10/10\n",
      "105991/105991 [==============================] - 92s - loss: 0.0523 - val_loss: 0.0253\n",
      "[ 0.15383961  0.15448292]\n"
     ]
    }
   ],
   "source": [
    "#with dropout\n",
    "in_neurons = 2  \n",
    "out_neurons = 2  \n",
    "hidden_neurons = 20\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=in_neurons, return_sequences=False)) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(output_dim=out_neurons, input_dim=hidden_neurons))\n",
    "model.add(Activation(\"softmax\"))  \n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\") \n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = train_test_split(data)  # retrieve data\n",
    "model.fit(train_dataset, train_labels, batch_size=450, nb_epoch=10, validation_split=0.05)  \n",
    "\n",
    "predicted = model.predict(test_dataset)  \n",
    "rmse = np.sqrt(((predicted - test_labels) ** 2).mean(axis=0))\n",
    "\n",
    "print rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 11000 samples\n",
      "Epoch 1/10\n",
      "99000/99000 [==============================] - 70s - loss: 0.0569 - acc: 0.8880 - val_loss: 0.0138 - val_acc: 0.9969\n",
      "Epoch 2/10\n",
      "99000/99000 [==============================] - 70s - loss: 0.0544 - acc: 0.9565 - val_loss: 0.0124 - val_acc: 0.9973\n",
      "Epoch 3/10\n",
      "99000/99000 [==============================] - 74s - loss: 0.0543 - acc: 0.9777 - val_loss: 0.0123 - val_acc: 0.9980\n",
      "Epoch 4/10\n",
      "99000/99000 [==============================] - 77s - loss: 0.0543 - acc: 0.9879 - val_loss: 0.0122 - val_acc: 0.9982\n",
      "Epoch 5/10\n",
      "99000/99000 [==============================] - 81s - loss: 0.0542 - acc: 0.9901 - val_loss: 0.0122 - val_acc: 0.9983\n",
      "Epoch 6/10\n",
      "99000/99000 [==============================] - 81s - loss: 0.0542 - acc: 0.9918 - val_loss: 0.0122 - val_acc: 0.9984\n",
      "Epoch 7/10\n",
      "99000/99000 [==============================] - 83s - loss: 0.0542 - acc: 0.9946 - val_loss: 0.0122 - val_acc: 0.9985\n",
      "Epoch 8/10\n",
      "99000/99000 [==============================] - 85s - loss: 0.0542 - acc: 0.9949 - val_loss: 0.0121 - val_acc: 0.9985\n",
      "Epoch 9/10\n",
      "99000/99000 [==============================] - 86s - loss: 0.0542 - acc: 0.9934 - val_loss: 0.0122 - val_acc: 0.9986\n",
      "Epoch 10/10\n",
      "99000/99000 [==============================] - 86s - loss: 0.0542 - acc: 0.9952 - val_loss: 0.0121 - val_acc: 0.9987\n",
      "[ 0.15264578  0.15463722]\n"
     ]
    }
   ],
   "source": [
    "in_neurons = 2  \n",
    "out_neurons = 2  \n",
    "hidden_neurons = 20\n",
    "batch_size = 500\n",
    "time_steps = 100\n",
    "training_size = 110000 # needs to be divisable by batch_size\n",
    "test_size = 37000 # needs to be divisable by batch_size\n",
    "\n",
    "model3 = Sequential()  \n",
    "model3.add(LSTM(output_dim=hidden_neurons, batch_input_shape=(batch_size, time_steps, in_neurons), stateful=True, return_sequences=False)) \n",
    "model3.add(Dense(output_dim=out_neurons, activation='softmax'))\n",
    "\n",
    "model3.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\") \n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = train_test_split(data)  # retrieve data\n",
    "model3.fit(train_dataset[0:training_size], train_labels[0:training_size], batch_size=batch_size, nb_epoch=10, validation_split=0.1, show_accuracy=True)  \n",
    "\n",
    "predicted = model3.predict(test_dataset[0:test_size], batch_size=batch_size)  \n",
    "rmse = np.sqrt(((predicted - test_labels[0:test_size]) ** 2).mean(axis=0))\n",
    "\n",
    "print rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: x has 500 rows but z has 128 rows\nApply node that caused the error: Gemm{inplace}(Dot22.0, TensorConstant{0.20000000298}, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, TensorConstant{0.20000000298})\nToposort index: 10\nInputs types: [TensorType(float32, matrix), TensorType(float32, scalar), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, scalar)]\nInputs shapes: [(128, 20), (), (500, 20), (20, 20), ()]\nInputs strides: [(80, 4), (), (80, 4), (80, 4), ()]\nInputs values: ['not shown', array(0.20000000298023224, dtype=float32), 'not shown', 'not shown', array(0.20000000298023224, dtype=float32)]\nOutputs clients: [[Elemwise{Composite{(clip((i0 + i1 + i2), i3, i4) * tanh(i5))}}(TensorConstant{(1, 1) of 0.5}, Elemwise{mul,no_inplace}.0, Gemm{inplace}.0, TensorConstant{(1, 1) of 0}, TensorConstant{(1, 1) of 1}, Elemwise{Composite{((clip((i0 + i1 + i2), i3, i4) * i5) + (clip((i6 + i7 + i8), i3, i4) * tanh((i9 + i10))))}}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{1}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{1}.0, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0)\nToposort index: 31\nInputs types: [TensorType(int64, scalar), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(int64, scalar), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, row), TensorType(float32, row), TensorType(float32, row), TensorType(float32, row)]\nInputs shapes: [(), (100, 128, 2), (2, 500, 20), (2, 500, 20), (), (2, 20), (20, 20), (2, 20), (20, 20), (2, 20), (20, 20), (2, 20), (20, 20), (1, 20), (1, 20), (1, 20), (1, 20)]\nInputs strides: [(), (8, 800, 4), (40000, 80, 4), (40000, 80, 4), (), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4)]\nInputs values: [array(100), 'not shown', 'not shown', 'not shown', array(100), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\nOutputs clients: [[Subtensor{int64}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0)], [Subtensor{int64}(forall_inplace,cpu,scan_fn}.1, ScalarFromTensor.0)], [InplaceDimShuffle{0,1,2}(forall_inplace,cpu,scan_fn}.2)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-710ef002ddaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#rmse = np.sqrt(((predicted - test_labels[0:500]) ** 2).mean(axis=0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, batch_size, verbose)\u001b[0m\n\u001b[0;32m    659\u001b[0m         '''\n\u001b[0;32m    660\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/felix/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:4316)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/felix/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:4193)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape mismatch: x has 500 rows but z has 128 rows\nApply node that caused the error: Gemm{inplace}(Dot22.0, TensorConstant{0.20000000298}, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, TensorConstant{0.20000000298})\nToposort index: 10\nInputs types: [TensorType(float32, matrix), TensorType(float32, scalar), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, scalar)]\nInputs shapes: [(128, 20), (), (500, 20), (20, 20), ()]\nInputs strides: [(80, 4), (), (80, 4), (80, 4), ()]\nInputs values: ['not shown', array(0.20000000298023224, dtype=float32), 'not shown', 'not shown', array(0.20000000298023224, dtype=float32)]\nOutputs clients: [[Elemwise{Composite{(clip((i0 + i1 + i2), i3, i4) * tanh(i5))}}(TensorConstant{(1, 1) of 0.5}, Elemwise{mul,no_inplace}.0, Gemm{inplace}.0, TensorConstant{(1, 1) of 0}, TensorConstant{(1, 1) of 1}, Elemwise{Composite{((clip((i0 + i1 + i2), i3, i4) * i5) + (clip((i6 + i7 + i8), i3, i4) * tanh((i9 + i10))))}}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{1}.0, Subtensor{int64:int64:int8}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{1}.0, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, <TensorType(float32, matrix)>, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0, InplaceDimShuffle{x,0}.0)\nToposort index: 31\nInputs types: [TensorType(int64, scalar), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(int64, scalar), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, row), TensorType(float32, row), TensorType(float32, row), TensorType(float32, row)]\nInputs shapes: [(), (100, 128, 2), (2, 500, 20), (2, 500, 20), (), (2, 20), (20, 20), (2, 20), (20, 20), (2, 20), (20, 20), (2, 20), (20, 20), (1, 20), (1, 20), (1, 20), (1, 20)]\nInputs strides: [(), (8, 800, 4), (40000, 80, 4), (40000, 80, 4), (), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4), (80, 4)]\nInputs values: [array(100), 'not shown', 'not shown', 'not shown', array(100), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\nOutputs clients: [[Subtensor{int64}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0)], [Subtensor{int64}(forall_inplace,cpu,scan_fn}.1, ScalarFromTensor.0)], [InplaceDimShuffle{0,1,2}(forall_inplace,cpu,scan_fn}.2)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "predicted = model3.predict(test_dataset[0:500])  \n",
    "#rmse = np.sqrt(((predicted - test_labels[0:500]) ** 2).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37090, 100, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105991 samples, validate on 5579 samples\n",
      "Epoch 1/10\n",
      "105991/105991 [==============================] - 14758s - loss: 0.0591 - val_loss: 0.0260\n",
      "Epoch 2/10\n",
      "105991/105991 [==============================] - 14351s - loss: 0.0520 - val_loss: 0.0259\n",
      "Epoch 3/10\n",
      " 24300/105991 [=====>........................] - ETA: 10753s - loss: 0.0516"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e6258712d9ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#(X_train, y_train), (X_test, y_test) = train_test_split(data)  # retrieve data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m450\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_neurons = 2  \n",
    "out_neurons = 2  \n",
    "h1 = 300\n",
    "h2 = 500\n",
    "h3 = 200\n",
    "\n",
    "model2 = Sequential()  \n",
    "\n",
    "model2.add(LSTM(output_dim=h1, input_dim=in_neurons, return_sequences=True))  \n",
    "model2.add(LSTM(output_dim=h2, input_dim=h1, return_sequences=True))  \n",
    "model2.add(Dropout(0.5))  \n",
    "model2.add(LSTM(output_dim=h3, input_dim=h2, return_sequences=False))  \n",
    "model2.add(Dropout(0.5))  \n",
    "model2.add(Dense(output_dim=out_neurons, input_dim=h3))\n",
    "model2.add(Activation(\"softmax\"))  \n",
    "model2.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\") \n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = train_test_split(data)  # retrieve data\n",
    "model2.fit(train_dataset, train_labels, batch_size=450, nb_epoch=10, validation_split=0.05)  \n",
    "\n",
    "predicted = model2.predict(test_dataset)  \n",
    "rmse = np.sqrt(((predicted - test_labels) ** 2).mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
