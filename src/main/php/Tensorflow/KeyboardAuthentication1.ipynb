{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import Xlib.display as display\n",
    "import gtk, pygtk\n",
    "import csv\n",
    "import numpy as np\n",
    "import ConfigParser\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = 3.5e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read write project path\n",
    "#TODO: configure this file according to your path: BehavioralAuthentication/src/main/php/configuration.cfg\n",
    "config = ConfigParser.RawConfigParser()\n",
    "config.read('configuration.cfg')\n",
    "\n",
    "#project_dir_path = config.get('Section1', 'project_dir')\n",
    "#resource_path = project_dir_path + \"src/test/resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window = gtk.Window()\n",
    "screen = window.get_screen()\n",
    "\n",
    "x_size = screen.get_width()\n",
    "y_size = screen.get_height()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new\n",
    "\n",
    "#ted dunning model\n",
    "\n",
    "def extractFeatures(filename, n_last_timestamps_as_features = 200, normalize = True, bias = True):\n",
    "    queue = {}\n",
    "    \n",
    "    diffTime = {}\n",
    "    oldTime = -1.0\n",
    "    \n",
    "    maxX = 0.0\n",
    "    \n",
    "    x_width = float(screen.get_width()) \n",
    "    y_width = float(screen.get_height()) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    x_attribute = -1\n",
    "    y_attribute = -1\n",
    "    i = 0\n",
    "    r = 0\n",
    "    \n",
    "    record_number = len(list(csv.reader(open(filename))))\n",
    "    print record_number\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "         spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')   \n",
    "            \n",
    "         for row in spamreader:\n",
    "            \n",
    "            feature_num = len(row)\n",
    "            \n",
    "            record_size = feature_num * n_last_timestamps_as_features\n",
    "            data_size = record_number - n_last_timestamps_as_features - 1\n",
    "            \n",
    "            if (i==0):\n",
    "                \n",
    "                data = np.zeros((data_size, record_size))\n",
    "                target = np.zeros((data_size, feature_num))\n",
    "                \n",
    "                #find mouse position coordinate columns in the file\n",
    "                for a in range(feature_num):\n",
    "                    if (row[a] == \"mouse_X\"):\n",
    "                        x_attribute = a\n",
    "                    if (row[a] == \"mouse_Y\"):\n",
    "                        y_attribute = a\n",
    "                        \n",
    "            else:\n",
    "                #normalize mouse coordinates\n",
    "                if (normalize):\n",
    "                     row[x_attribute] = float(row[x_attribute]) / x_width   \n",
    "                     row[y_attribute] = float(row[y_attribute]) / y_width\n",
    "                \n",
    "                \n",
    "                if (i <= n_last_timestamps_as_features):\n",
    "                    data [0,(i-1)*feature_num : i*feature_num] = row\n",
    "                    \n",
    "                elif(i == n_last_timestamps_as_features + 1):\n",
    "                    target[0,:] = row\n",
    "                    \n",
    "                    r = r + 1\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    data[r, 0 : (n_last_timestamps_as_features - 1) * feature_num] = \\\n",
    "                        data[(r - 1), feature_num : (n_last_timestamps_as_features) * feature_num]\n",
    "                    data[r, (n_last_timestamps_as_features - 1) * feature_num : (n_last_timestamps_as_features) * feature_num] = \\\n",
    "                        target[(r-1),:]\n",
    "                        \n",
    "                    target[r,:] = row                   \n",
    "                \n",
    "                    r = r + 1\n",
    "            i = i + 1\n",
    "            \n",
    "    if (bias):\n",
    "        data = np.hstack((data, np.ones((data.shape[0], 1), dtype=data.dtype)))\n",
    "    \n",
    "    \n",
    "    print(\"original shape: \" + str(data.shape))\n",
    "\n",
    "    return ((data, target))\n",
    "            \n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "original shape: (7, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  1.,   2.,   1.],\n",
       "        [  3.,   4.,   1.],\n",
       "        [  5.,   6.,   1.],\n",
       "        [  7.,   8.,   1.],\n",
       "        [  9.,  10.,   1.],\n",
       "        [ 11.,  12.,   1.],\n",
       "        [ 13.,  14.,   1.]]), array([[  3.,   4.],\n",
       "        [  5.,   6.],\n",
       "        [  7.,   8.],\n",
       "        [  9.,  10.],\n",
       "        [ 11.,  12.],\n",
       "        [ 13.,  14.],\n",
       "        [ 15.,  16.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractFeatures(\"/home/felix/mousetracking/BehavioralAuthentication/src/main/java/features4.txt\", \\\n",
    "                n_last_timestamps_as_features = 1, normalize = False, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateModelData((data1,target1), n_last_timestamps_as_features = 200, \n",
    "                      v = False, build_all_model = True, p_train_general = 1.0, \n",
    "                      stddev_factor = 1.0, returnSmoothed = False):\n",
    "    \n",
    "    N_total = data1.shape[0]\n",
    "    N_total_test = N_total * 0.4\n",
    "    \n",
    "    #(data,target) = getMovementsOnly(data1, target1)\n",
    "    (data,target) = (data1, target1)\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    print data.shape\n",
    "    print target.shape\n",
    "    \n",
    "    p_train = 0.6\n",
    "    X_train = data[0:(N * p_train_general * p_train),:]\n",
    "    y_train = target[0:(N * p_train_general * p_train),:]\n",
    "    X_test = data[((N * p_train) + n_last_timestamps_as_features):N,:]\n",
    "    y_test = target[((N * p_train) + n_last_timestamps_as_features):N,:]\n",
    "    \n",
    "    N_total_test = X_test.shape[0]\n",
    "    \n",
    "\n",
    "    regr = linear_model.LinearRegression(normalize=False)\n",
    "    \n",
    "    dist_train = getDistance(y_train)\n",
    "    \n",
    "    #impossible threshold\n",
    "    impossible_threshold = np.array([stddev_factor, stddev_factor])\n",
    "\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train);\n",
    "    \n",
    "    #training error\n",
    "    dist_train = getDistance(y_train)\n",
    "    \n",
    "    prediction_train = regr.predict(X_train)\n",
    "    \n",
    "    #target y is constrained to be within in [0,1]\n",
    "    prediction_train[prediction_train < 0.0] = 0\n",
    "    prediction_train[prediction_train > 1.0] = 1\n",
    "    \n",
    "    diff_train = np.abs(prediction_train - y_train)\n",
    "    diff_x_train = diff_train[:,0]\n",
    "    diff_y_train = diff_train[:,1]\n",
    "    \n",
    "    train_error_mse = (np.sum((np.power(diff_x_train, 2) + np.power(diff_y_train, 2))) / diff_x_train.shape[0])\n",
    "    train_error_dist = (np.sum((np.power(diff_x_train, 2) + np.power(diff_y_train, 2))) / dist_train)\n",
    "    \n",
    "    (smoothed_time_error_train, smoothed_distance_error_train, N_smoothed_train) = \\\n",
    "    get_smoothed_result(X_train,y_train, diff_train, impossible_threshold, v = False) \n",
    "   \n",
    "\n",
    "    #test error\n",
    "    dist_test = getDistance(y_test)\n",
    "    \n",
    "    prediction_test = regr.predict(X_test)\n",
    "    \n",
    "    #target y is constrained to be within in [0,1]\n",
    "    prediction_test[prediction_test < 0.0] = 0\n",
    "    prediction_test[prediction_test > 1.0] = 1\n",
    "    \n",
    "    diff_test = np.abs(prediction_test - y_test)        \n",
    "    diff_x_test = diff_test[:,0]\n",
    "    diff_y_test = diff_test[:,1]\n",
    "    \n",
    "    test_error_mse = (np.sum(np.power(diff_x_test, 2) + np.power(diff_y_test, 2)) / diff_x_test.shape[0])\n",
    "    test_error_dist = (np.sum(np.power(diff_x_test, 2) + np.power(diff_y_test, 2)) / dist_test)\n",
    "    \n",
    "    (smoothed_time_error_test, smoothed_distance_error_test, N_smoothed) = \\\n",
    "    get_smoothed_result(X_test,y_test, diff_test, impossible_threshold, v = v) \n",
    "    \n",
    "    if (v):\n",
    "        \n",
    "        print(\"threshold: \" + str(impossible_threshold))\n",
    "           \n",
    "        #Prediction of x-coordinate of the cursor over time\n",
    "        plt.plot(y_test[:,0])\n",
    "        plt.plot(prediction_test[:,0])\n",
    "        plt.ylabel('relative postion - x coordinate of the cursor')\n",
    "        plt.title(\"regression - x\")\n",
    "        plt.show()\n",
    "        \n",
    "        #Prediction of y-coordinate of the cursor over time\n",
    "        plt.plot(y_test[:,1])\n",
    "        plt.plot(prediction_test[:,1])\n",
    "        plt.ylabel('relative postion - y coordinate of the cursor')\n",
    "        plt.title(\"regression - y\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"test - MSE: %.10f\" % test_error_mse)\n",
    "        print(\"test - MSE - dist: %.10f\" % test_error_dist)\n",
    "        \n",
    "        print (\"test - smoothed MSE: \" + str(smoothed_time_error_test))\n",
    "        print (\"test - smoothed MSE per dist: \" + str(smoothed_distance_error_test))\n",
    "        \n",
    "        #Test error over time\n",
    "        threshold_line = np.ones(diff_x_test.shape) * impossible_threshold[0]        \n",
    "        plt.plot(diff_x_test)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Test error')\n",
    "        plt.title(\"ErrorPerTestTime - x\")\n",
    "        plt.show()\n",
    "        threshold_line = np.ones(diff_y_test.shape) * impossible_threshold[1]        \n",
    "        plt.plot(diff_y_test)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Test error')\n",
    "        plt.title(\"ErrorPerTestTime - y\")\n",
    "        plt.show()\n",
    "        \n",
    "        #Training error over time\n",
    "        threshold_line = np.ones(diff_x_train.shape) * impossible_threshold[0]\n",
    "        plt.plot(diff_x_train)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Training error')\n",
    "        plt.title(\"ErrorPerTrainTime - x\")\n",
    "        plt.show()\n",
    "        \n",
    "        threshold_line = np.ones(diff_y_train.shape) * impossible_threshold[1]\n",
    "        plt.plot(diff_y_train)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('Training error')\n",
    "        plt.title(\"ErrorPerTrainTime - y\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"train - MSE: %.10f\" % train_error_mse)\n",
    "        print(\"train - MSE - dist: %.10f\" % train_error_dist)    \n",
    "        \n",
    "        print (\"train - smoothed MSE: \" + str(smoothed_time_error_train))\n",
    "        print (\"train - smoothed MSE per dist: \" + str(smoothed_distance_error_train))\n",
    "    \n",
    "        \n",
    "    if (returnSmoothed):\n",
    "        return((smoothed_time_error_test, smoothed_distance_error_test, float(N_smoothed) / float(N_total_test)))\n",
    "    \n",
    "    if (build_all_model):   \n",
    "        final_regr = linear_model.LinearRegression(normalize=True)\n",
    "    \n",
    "        final_regr.fit(data, target);\n",
    "    \n",
    "        return ((final_regr, impossible_threshold))\n",
    "        #return (regr, two_sigma)\n",
    "    else:\n",
    "        return test_error_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateModel(dataFile, n_last_timestamps_as_features = 200, v = False, stddev_factor = 0.0):\n",
    "    (data,target) = extractFeatures(dataFile, n_last_timestamps_as_features)\n",
    "    \n",
    "    return generateModelData((data,target), n_last_timestamps_as_features = n_last_timestamps_as_features, \n",
    "                             v = v, stddev_factor = stddev_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMovementsOnly(data, target):\n",
    "    moveOnlyData = []\n",
    "    moveOnlyTarget = []\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    for l in range(data.shape[0]):\n",
    "        if (first):\n",
    "            \n",
    "            #print data[l]\n",
    "            \n",
    "            moveOnlyData = [data[l]]\n",
    "            moveOnlyTarget = [target[l]]\n",
    "            first = False\n",
    "        else:\n",
    "            \n",
    "            #print (\"new: \" + str(data[l]))\n",
    "            #print (\"old: \" + str(data[l]))\n",
    "            \n",
    "            if (np.any(data[l] != data[l-1]) or np.any(target[l] != target[l-1])):\n",
    "                moveOnlyData = np.concatenate((moveOnlyData, [data[l]]), axis=0)\n",
    "                moveOnlyTarget = np.concatenate((moveOnlyTarget, [target[l]]), axis=0)\n",
    "\n",
    "    return (np.matrix(moveOnlyData), np.matrix(moveOnlyTarget))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean (p1, p2):\n",
    "    return np.sqrt(np.sum(np.power(p1 - p2, 2)))\n",
    "\n",
    "def getDistance(target):\n",
    "    distance = 0.0\n",
    "    for t in np.arange(1, target.shape[0], 1):\n",
    "        distance += euclidean(target[t,:], target[(t-1),:])\n",
    "    \n",
    "    return distance    \n",
    "\n",
    "def getSmoothedDistance(target, isOverThreshold):\n",
    "    \n",
    "    distance = 0.0\n",
    "    for t in np.arange(1, target.shape[0], 1):\n",
    "        last_point_x = target[t-1,0]\n",
    "        last_point_y = target[t-1,1]\n",
    "        if (isOverThreshold[t] == False):\n",
    "            distance += euclidean(target[t,:], np.array([last_point_x,last_point_y])) \n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_smoothed_result(newData, newTarget, diff, threshold, v = False):\n",
    "    \n",
    "    diff_smoothed = np.copy(diff)\n",
    "    \n",
    "    N_smoothed = newTarget.shape[0]\n",
    "    \n",
    "    is_over_threshold = np.logical_or((diff_smoothed[:,0] > threshold[0]),(diff_smoothed[:,1] > threshold[1]))\n",
    "    \n",
    "    newdist = getSmoothedDistance(newTarget, is_over_threshold)\n",
    "    \n",
    "    for l in range(diff_smoothed.shape[0]):\n",
    "        if (is_over_threshold[l]):\n",
    "            diff_smoothed[l,0] = 0\n",
    "            diff_smoothed[l,1] = 0\n",
    "            N_smoothed -= 1\n",
    "            \n",
    "    if (v):\n",
    "        plt.plot(diff_smoothed[:,0])\n",
    "        plt.ylabel('prediction x error -smoothed')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(diff_smoothed[:,1])\n",
    "        plt.ylabel('prediction y error -smoothed')\n",
    "        plt.show()\n",
    "    \n",
    "    time_error = 0.0\n",
    "    if (N_smoothed > 0.0):\n",
    "        time_error = np.sum(np.power(diff_smoothed,2)) / N_smoothed\n",
    "    \n",
    "    distance_error = 0.0\n",
    "    if (newdist > 0.0):\n",
    "        distance_error = np.sum(np.power(diff_smoothed,2)) / newdist\n",
    "    \n",
    "    return ((time_error,distance_error, N_smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showStats(file, m, threshold, n_last_timestamps_as_features = 200, normalize = True, v = True):\n",
    "    (fileX1, fileY1) = extractFeatures(file, \n",
    "                                     n_last_timestamps_as_features = n_last_timestamps_as_features, \n",
    "                                     normalize = normalize) \n",
    "    \n",
    "    return showStatsData((fileX1, fileY1), m, threshold, n_last_timestamps_as_features = n_last_timestamps_as_features, v = v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000086145\n"
     ]
    }
   ],
   "source": [
    "t = 1000000.0\n",
    "    \n",
    "for i in range(1000001):\n",
    "    t += 0.000001\n",
    "\n",
    "t -= 1000000.0\n",
    "\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showStatsData((fileX, fileY), m, threshold, n_last_timestamps_as_features = 200, v = True):\n",
    "    \n",
    "    (newData, newTarget) = getMovementsOnly(fileX, fileY)   \n",
    "    \n",
    "    if newData.shape[0] <= 1: return(0.0, 0.0, 0, 1)\n",
    "    \n",
    "    #distance\n",
    "    dist = getDistance(newTarget)\n",
    "    \n",
    "    prediction_sample = m.predict(newData)\n",
    "    \n",
    "    #target y is constrained to be within in [0,1]\n",
    "    prediction_sample[prediction_sample < 0.0] = 0\n",
    "    prediction_sample[prediction_sample > 1.0] = 1\n",
    "\n",
    "    ErrorPerTestTime = np.sum(np.absolute(prediction_sample - newTarget),1);\n",
    "    \n",
    "    threshold_line = np.ones(ErrorPerTestTime.shape) * (threshold[0] + threshold[1])\n",
    "    \n",
    "    if (v):\n",
    "        #error of sample\n",
    "        plt.plot(ErrorPerTestTime)\n",
    "        plt.plot(threshold_line)\n",
    "        plt.ylabel('prediction error')\n",
    "        plt.show()\n",
    "\n",
    "        #Prediction of x-coordinate of the cursor over time\n",
    "        plt.plot(prediction_sample[:,0])\n",
    "        plt.plot(newData[:,0])\n",
    "        plt.ylabel('relative x-coordinate')\n",
    "        plt.title(\"Regression for x-coordinate\")    \n",
    "        plt.show()\n",
    "\n",
    "        #Prediction of y-coordinate of the cursor over time\n",
    "        plt.plot(prediction_sample[:,1])\n",
    "        plt.plot(newData[:,1])\n",
    "        plt.ylabel('relative y-coordinate')\n",
    "        plt.title(\"Regression for y-coordinate\")\n",
    "        plt.show()\n",
    "\n",
    "    diff = np.absolute(prediction_sample - newTarget)\n",
    "    \n",
    "    N = newTarget.shape[0]\n",
    "    \n",
    "    if (v):\n",
    "        print (\"MSE per 0.03s: \" + str(np.sum(np.power(diff,2)) / N))\n",
    "        print (\"MSE per pixel move: \" + str(np.sum(np.power(diff,2)) / dist))\n",
    "        print (\"number of records: \" + str(N))\n",
    "\n",
    "        print (\"\\nmax - |x+y|: \" + str(np.max(diff)))\n",
    "        print (\"max - squared: \" + str(np.max(np.power(diff,2))))\n",
    "    \n",
    "    (time_error, distance_error, N_smoothed) = get_smoothed_result(newData, newTarget, diff, threshold)    \n",
    "    \n",
    "    if (v):\n",
    "        print (\"smoothed MSE: \" + str(time_error))\n",
    "        print (\"smoothed MSE per dist: \" + str(distance_error))\n",
    "    \n",
    "    if (time_error > theta): \n",
    "        if (v):\n",
    "            print (\"This is not Felix - You are unauthorized!\")\n",
    "        return (time_error, distance_error, N_smoothed, 0)\n",
    "    else:\n",
    "        if (v):\n",
    "            print (\"This is Felix - Welcome to this awesome computer!\")  \n",
    "        return (time_error, distance_error, N_smoothed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186051\n",
      "original shape: (185950, 201)\n",
      "(185950, 2)\n",
      "(25126, 201)\n",
      "(25126, 2)\n"
     ]
    }
   ],
   "source": [
    "p_train = 0.6\n",
    "p_valid = 0.2\n",
    "#p_test = 0.2\n",
    "\n",
    "rr_steps = 100\n",
    "\n",
    "(data,target) = extractFeatures(\"/home/felix/mousetracking/BehavioralAuthentication/src/main/java/features3.txt\", \\\n",
    "                                rr_steps)\n",
    "\n",
    "print(target.shape)\n",
    "\n",
    "data = np.matrix(data, dtype=\"float32\")\n",
    "target = np.matrix(target, dtype=\"float32\")\n",
    "\n",
    "N = data.shape[0]\n",
    "\n",
    "train_dataset = data[0:(N * p_train),:]\n",
    "train_labels = target[0:(N * p_train),:]\n",
    "valid_dataset = data[((N * p_train) + rr_steps):((N * (p_train + p_valid))),:]\n",
    "valid_labels = target[((N * p_train) + rr_steps):((N * (p_train + p_valid))),:]\n",
    "test_dataset = data[((N * (p_train + p_valid)) + rr_steps):N,:]\n",
    "test_labels = target[((N * (p_train + p_valid)) + rr_steps):N,:]\n",
    "\n",
    "(train_dataset,train_labels) = getMovementsOnly(train_dataset, train_labels)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "num_features = train_dataset.shape[1]\n",
    "num_labels = train_labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  \n",
    "  return np.sum((np.square(np.matrix(predictions) - np.matrix(labels))).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layers = 1224\n",
    "beta = 5e-4 #l2 regularization parameter\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, num_features))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # Variables.\n",
    "  w1 = tf.Variable(\n",
    "    tf.truncated_normal([num_features, hidden_layers], seed=1))\n",
    "  b1 = tf.Variable(tf.zeros([hidden_layers]))\n",
    "    \n",
    "  w2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_layers, num_labels], seed=2))\n",
    "  b2 = tf.Variable(tf.zeros([num_labels]))   \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1), w2) + b2 \n",
    "\n",
    "  loss = tf.reduce_mean(tf.square(tf.nn.softmax(logits) - tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(w1) + tf.nn.l2_loss(b1) +\n",
    "                  tf.nn.l2_loss(w2) + tf.nn.l2_loss(b2))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += beta * regularizers\n",
    "  \n",
    "  # Optimizer.\n",
    "  batch = tf.Variable(0)\n",
    "\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    0.01,                # Base learning rate.\n",
    "    batch * batch_size,  # Current index into the dataset.\n",
    "    train_dataset.shape[0],          # Decay step.\n",
    "    0.95,                # Decay rate.\n",
    "    staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                     0.9).minimize(loss,\n",
    "                                                   global_step=batch) \n",
    "\n",
    "\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), w2) + b2 )\n",
    "    \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w2) + b2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 48.467491\n",
      "Minibatch accuracy: 0.947044\n",
      "Validation accuracy: 0.853530\n",
      "Minibatch loss at step 500: 46.141380\n",
      "Minibatch accuracy: 0.710467\n",
      "Validation accuracy: 0.483996\n",
      "Minibatch loss at step 1000: 44.189671\n",
      "Minibatch accuracy: 0.599737\n",
      "Validation accuracy: 0.483845\n",
      "Minibatch loss at step 1500: 42.412067\n",
      "Minibatch accuracy: 0.232574\n",
      "Validation accuracy: 0.483634\n",
      "Minibatch loss at step 2000: 41.186825\n",
      "Minibatch accuracy: 0.490554\n",
      "Validation accuracy: 0.482520\n",
      "Minibatch loss at step 2500: 40.269745\n",
      "Minibatch accuracy: 0.958225\n",
      "Validation accuracy: 0.481822\n",
      "Minibatch loss at step 3000: 39.093845\n",
      "Minibatch accuracy: 0.574267\n",
      "Validation accuracy: 0.481039\n",
      "Test accuracy: 0.449231\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %f\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %f\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %f\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  \n",
    "  rmse = np.sqrt((np.square(test_prediction.eval() - test_labels)).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47994742  0.4678472 ]]\n"
     ]
    }
   ],
   "source": [
    "print rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.constant_op import constant\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "To classify images using a bidirectional reccurent neural network, we consider every image row as a sequence of pixels.\n",
    "Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input =  num_features# MNIST data input (img shape: 28*28)\n",
    "n_steps = rr_steps # timesteps\n",
    "n_hidden = hidden_layers # hidden layer num of features\n",
    "n_classes = num_labels # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "# Tensorflow LSTM cell requires 2x n_hidden length (state & cell)\n",
    "istate_fw = tf.placeholder(\"float\", [None, 2*n_hidden])\n",
    "istate_bw = tf.placeholder(\"float\", [None, 2*n_hidden])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, 2*n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([2*n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def BiRNN(_X, _istate_fw, _istate_bw, _weights, _biases):\n",
    "\n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs = rnn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, _X,\n",
    "                                             initial_state_fw=_istate_fw,\n",
    "                                             initial_state_bw=_istate_bw)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']\n",
    "\n",
    "pred = BiRNN(x, istate_fw, istate_bw, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(tf.nn.softmax(pred) - y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_xs = batch_xs.reshape((batch_size, n_steps, n_input))\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                       istate_fw: np.zeros((batch_size, 2*n_hidden)),\n",
    "                                       istate_bw: np.zeros((batch_size, 2*n_hidden))})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                                istate_fw: np.zeros((batch_size, 2*n_hidden)),\n",
    "                                                istate_bw: np.zeros((batch_size, 2*n_hidden))})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                             istate_fw: np.zeros((batch_size, 2*n_hidden)),\n",
    "                                             istate_bw: np.zeros((batch_size, 2*n_hidden))})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                                             istate_fw: np.zeros((test_len, 2*n_hidden)),\n",
    "                                                             istate_bw: np.zeros((test_len, 2*n_hidden))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
